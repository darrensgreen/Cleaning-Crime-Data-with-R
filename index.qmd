---
title: "A Guide to Crime Data Cleaning & Preparation in RStudio"
author: "Darren Green"
date: "today"
format: 
  html:
    toc: true
    toc-location: left
    theme: cosmo
bibliography: references.bib
---

## Introduction

Welcome! This guide provides a standard workflow for cleaning, preparing, and exploring crime data using R. After receiving requests to help people learn R, I developed this guide as a supplementary resource, replacing emails containing random chunks of code. I have structured this guide based on the order of steps I have found to be effective for me when analysing crime data. Specifically, I have found the following workflow to be most effective;

1.  Setting up my library and project settings

2.  Loading my data

3.  Reviewing my data

4.  Transform my data in this specific order

    -   Format column names

    -   Format dates

    -   Format remaining variables (renaming categorical variables, addressing NA values or ensuring R is recognising numeric variables as numeric)

    -   Once the above steps have been completed, my data is suitably structured for creating dummy variables or calculating new variables.

I have found this workflow to be reliable for analysing data related to crime, road safety, offender recidivism, as well as other content obtained online.

The purpose of this guide is just to get you started and provide a structure that will hopefully assist you in your data cleaning process. This is not a comprehensive or definitive guide. As you proceed through your R journey, you will see that the flexibility and capabilities of R prevent this from being possible in a single guide. I find that I continue to refine and adjust my coding practices as I come across new problems and see how other members of the R community set up their code. Nonetheless, I hope you find this to be a useful starting point.

You may also note that some of the code I present in this guide is not covered within the text. This has been done in an effort to keep it concise. However, I have added notes to any relevant code. If you would like to know more about any code, copying and pasting it into a common search engine provides more information.

I have structured this guide to allow you to work through each step with your own data set or use the Queensland Police Service (QPS) open dataset, which you can download directly from the [Open Data Portal](https://www.data.qld.gov.au/dataset/lga_reported_offences_number) [@queenslandpoliceservice2022]. A copy is also available on the GitHub page associated with this site.

Finally, some reassurance regarding the safety of your original data. You don't need to stress about corrupting your data. Unless you explicitly tell R to overwrite your CSV file, which requires you to call the relevant function and enter the correct arguments, any data manipulation you complete remains within the RStudio IDE.

------------------------------------------------------------------------

## Starting Your Project

Once you have opened R, click on the 'File' menu in the top left of your window and select 'New Project...'. This will open the New Project Wizard.

Figure 1: New Project Wizard

![](images/clipboard-122254317.png)

Select 'New Directory'. In the next window, you are presented with several options, including Quarto Website (used to create this page), book formats and journal article formats further down. Assuming this is your first project, we will keep things simple and select the top option, 'New Project'.

Figure 2: New Project Wizard - Project Type Screen

![](images/clipboard-1759991545.png)

In the next window, create a project name and select the folder you want your project created in.

**Note**: In the example below, a project folder called "Cleaning Crime Data with R" will be created within the Websites folder.

Figure 3: New Project Wizard - Create New Project Screen

![](images/clipboard-2758435978.png)

Click on 'Create Project' and R will set up the project for you.

## R File Types

Once you have created your project, you will need to create a file to write your code in. While some project types (e.g. book) automatically create the files for you, it is worth having a basic understanding of the file types available. The most common files I have used are the R Script and Quarto Documents. To create a new file, click on the blank page icon with the green plus symbol in the top left-hand corner of your window and select your desired file type.

Figure 4: Creating a New File Type

![](images/clipboard-3033828948.png)

Personally, I have a strong preference for Quarto Documents. My preference is based on several capabilities I have found beneficial to my research and work;

1.  I can break my code into manageable chunks (e.g. load libraries, load data, format data, extract burglary data, summarise burglary data, etc), as you will see in this guide.

2.  I can make notes to record my decision-making and thought process as I progress through my analysis. This website was written using a Quarto template.

3.  I can produce a single, self-contained document that includes my code, its output (like tables and plots), and my written analysis. This makes my work fully reproducible and easy to share with others as a polished report, a PDF, or even a website like this one.

## Adjusting Your View

If you are not used to markdown format, the initial layout may be confronting. Let's change your view to 'Visual' instead of 'Source' by clicking the 'Visual' button near the top left of your window.

Figure 5: The Source/Visual Toggle Button

![](images/clipboard-3470430110.png)

Figure 6: Source Layout

![](images/clipboard-2442749367.png)

Figure 7: Visual Layout

![](images/clipboard-2885730958.png)

## Creating and Labelling Code Chunks

In the next step, we will load a couple of libraries that contain useful functions to assist you in your data preparation process. Before we do that, we have you create a code chunk. In the top right of your window, you will see a c with a plus symbol to the left of it. Click on this and select the R format. Alternatively, you can use a keyboard shortcut Ctrl+Alt+I.

Figure 8: Insert a new code chunk

![](images/clipboard-1979597964.png)

A blank code chunk will import where your cursor is placed. It is good practice to give each chunk a unique label by typing `#| label: your-unique-label` on the line immediately after `{r}`. Labels must be unique, or you will get an error when you render your file. It is also best practice to avoid spaces; this will help you debug where errors occur when rendering your document. In the code chunk below, I have added a label called blank-chunk. To ensure the label is visible to you, I have commented this out by placing a `#` in front.

```{r}
#| label: blank-chunk
# #| label: blank-chunk # Deleting the first # will make this a chunk label.
# This is where your R code will go.
```

## Installing Packages

If this is your first time using R, you will be able to commence formatting and analysing data immediately, using base R commands. However, the true strength of R is realised by using packages. A package contains functions that greatly enhance the user experience. For most data wrangling tasks, the `tidyverse` is our best friend. Tidyverse is a meta package that loads many of the key packages we will be using, saving you from having to load each one individually. Once you have installed a package on your current device, it is placed in a library for you to call whenever you need. You don't have to install the package every time.

```{r}
#| label: installing-packages
#| eval: false

install.packages("tidyverse")
install.packages("explore")
install.packages("skimr")
```

## Loading Essential Libraries (Packages)

Once you have installed the required packages, during this session or previously, you need to load them using the `library()` command. Following this guide, you will need to load the required libraries every time you start a new R session.

```{r}
#| label: setup

# The 'tidyverse' is a collection of R packages for data science.
# It includes ggplot2 (for data visualisation), dplyr (for data manipulation),
# readr (for reading data), and more.
library(tidyverse)

# 'explore' is great for a quick, interactive first look at our data.
library(explore)

# 'skimr' provides additional summary data capability
library(skimr)


options(scipen = 999) # Removes scientific notation
theme_update(plot.title = element_text(hjust = 0.5)) # sets a project wide setting to center plot titles, saving you from having to type this line for every plot
```

Throughout this guide, we will cover the functions I use most frequently. However, these only represent a small proportion of the functions available. If you would like to know more about a package, you can try searching for details within common search engines or use the R `help()` function.

```{r}
#| label: R-help-function
#| eval: false

help(package = "dplyr") # Part of the tidyverse package
help(package = "lubridate") # Part of the tidyverse package
help(package = "tidyverse")
help(package = "explore")
help(package = "skimr")
```

------------------------------------------------------------------------

## Loading Data

Getting your data into R is the first step. Crime data often comes in CSV (Comma-Separated Values) files, and this will be the focus of this guide.

### Loading a Single CSV File

If your data is in a single CSV file located in your R project's working directory, you can load it easily with `read.csv()`. This method provides a sound starting point and is recommended. However, I have provided additional methods I have found to be useful in the following sections. Out of habit, I tend to use the base R `read.csv()` method. However, there is a `tidyverse` method, specifically `readr`, that provides additional functionality. For example, the `read_csv()` method includes the argument `trim_ws = TRUE` to remove white spaces on either side of strings, saving you formatting time down the track.

During this initial load, I tend to label my initial dataset 'raw' or similar. I then create a copy of the data set when I start formatting, so I can fall back on a pre-loaded dataset without having to load everything all over again. This is useful if you have a reasonably sized dataset to avoid waiting to reload your raw data when you make a formatting mistake (e.g. accidentally deleting all of your date data). However, it can also be problematic when working with very large datasets, taking up memory. Ultimately, I developed this habit when I first started learning R. Often, I would import the data once and then start working on it. Then I would make a mistake formatting and transforming my data, and would have to start all over again.

### Loading a CSV from your Project Directory

Once you have set up a project, you can place your data inside your directory. In this case, you can access using this method. However, this won't work if your data is stored in a central repository outside of your project folder.

```{r}
#| label: load-csv examples
#| eval: false

## Base R
# Replace "your_crime_data.csv" with the actual name of your file.
raw <- read.csv("your_crime_data.csv")

## Tidyverse method
raw <- read_csv("your_crime_data.csv")
```

```{r}
#| label: load-csv

# If you are using the supplied data set this would be 
raw <- read.csv("Qld LGA Reported Offences.csv")
```

### Loading a CSV from a Specific Directory

If the file is not in your R project directory, you need to provide the full path to the file. You can do this one of two ways. You can put the entire file path in each line of code, or you can leverage the `file.path()` function.

#### Option 1

```{r}
#| label: load-csv-path
#| eval: false

raw <- read.csv("C:/Users/YourUser/Documents/Data/your_crime_data.csv")
```

#### Option 2

This can be handy if you are loading multiple files or cannot store/access your data from a central repository such as OneDrive or GitHub, due to security limitations.

```{r}
#| label: load-csv-directory
#| eval: false

dir <- "C:/Users/YourUser/Documents/Data"

raw <- read.csv(file.path(dir, "your_csv_file_name.csv"))

```

**Tip**: A quick way to get your file path is to right-click on your file and select 'Copy Path'. Delete the file name and forward slash (/index.qmd in this example)

Figure 9: Obtaining your directory file path

![](images/clipboard-1018631641.png)

Copy file path raw: "C:/Users/*your_folder*/Documents/R Projects/Websites/Cleaning Crime Data with R/index.qmd"

Copy file path for importing data: "C:/Users/*your_folder*/Documents/R Projects/Websites/Cleaning Crime Data with R"

### Loading Multiple CSV Files at Once

Often, data is split across multiple files (e.g., one for each month). You can load and combine them all in one go using the `tidyverse` `map_dfr()` function. This code finds all files in a directory that end with `.csv` and stacks them into a single data frame. You may note that in this code chunk, I have used the `read_csv()` function call. I have done this for the sake of consistency, but I have previously used this method, using the `read.csv()` function.

```{r}
#| label: load-multi-csv
#| eval: false

# 1. Get a list of all CSV files in the "data_folder" directory.
files_to_load <- list.files(path = "data_folder/", pattern = "*.csv", full.names = TRUE)

# 1b. Get a list of all CSV files in the "data_folder" directory, using a directory variable.
files_to_load <- list.files(path = dir, pattern = "*.csv", full.names = TRUE)

# 2. Read each CSV and combine them into one data frame.
all_crime_data <- map_dfr(files_to_load, read_csv)
```

**Warning**: If the column names of your raw data are not in the first row, you will need to amend the above code. How you amend this is dependent on the layout of your raw data and beyond the scope of this guide.

------------------------------------------------------------------------

## Initial Data Exploration

Before cleaning, you need to understand your data's structure, identify missing values, and check data types.

The `explore()` function from the **explore** package is fantastic for an initial interactive overview.

```{r}
#| label: explore
#| eval: false

# This will generate an interactive report in your Viewer pane.
explore(raw)
```

For a quick summary in the console, `glimpse()` is your best bet. It shows you the data type of each column and the first few values.

```{r}
#| label: glimpse example code
#| eval: false

# Get a quick look at the structure of the data
glimpse(raw)
```

You can also double-click on your file in the environment window to open it in the RStudio viewer, reviewing just as you would a spreadsheet.

Figure 10: RStudio Viewer

![](images/clipboard-2226418659.png)

A common challenge when dealing with operational data is the impact of missing and NA values within the dataset. Some functions, such as `mean()`, will not handle NA values unless you explicitly ignore NA values by including an additional argument. Obviously, excluding NA values may adversely impact your data analysis. The `skimr` package can be very useful for summarising your data and identifying missing values.

```{r}
#| label: Assessing-for-data-type-and-missing-values

library(skimr)

skim(raw)
```

Obviously, being a publicly available dataset, it has already been cleaned for public consumption. In the next code chunk, we will temporarily modify the data set to include missing values and demonstrate the use of the `skimr` package.

```{r}
#| label: Create dataset with missing values

# Make a copy of the raw dataset to modify
temp <- raw

# Choose the number of NAs to add
n_missing_values <- 20

# Sample 20 random row numbers from the total number of rows
rows_to_replace <- sample(1:nrow(temp), size = n_missing_values)

# Add NA values to the LGA Names column
temp$LGA.Name[rows_to_replace] <- NA

# Now, skim the modified data
skim(temp)
```

Notice now that LGA.Name has 20 missing values, while Month.Year has none.

**Activity**: Review your data and identify blank cells, NA values, and other potential issues.

------------------------------------------------------------------------

## Data Cleaning and Manipulation

Now that your raw data has been loaded, we can start getting ready for analysis. As a general rule, I format the entire data set first because it reduces duplication. If I extract a subset (e.g. burglary offences) and then format that dataset, I will also have to format subsequent datasets, often using the exact code. However, if you create a subset and then decide you want to add or amend a variable for any reason, you can go back to your formatting section, rerun your code, and the amendment can be included in your subset data.

This is where we roll up our sleeves and get the data into a usable format. We'll use functions from the **dplyr** package.

As noted earlier, I like to leave my raw data as is. To do this, I will duplicate the raw data frame (commonly abbreviated as 'df') using the code below.

```{r}
#| label: create working df
#| eval: true

df <- raw
```

### Rename Columns

Column names are often messy. Let's make them clean and consistent (e.g., lower-case with underscores). You can call single columns, multiple columns consecutively, or explicitly rename columns. The method you use will come down to the dataset/s you have and your personal preferences. Here, I present two methods for changing column names. When I started, I predominantly used the base R `colnames()` method. I still use the base R method if I need to rename a large number of columns. However, I find I am using the tidyverse method more and more, particularly given that this allows me to pipe `|>` (chain together) multiple cleaning functions together (e.g. `new_df <- df |> rename() |> mutate(new_date = ...)`. Explaining pipes in detail is beyond the scope of this guide, but having an understanding that they can improve your workflow as you build your skills is worthwhile.

**Note**: when renaming columns, don't use spaces. This will complicate things down the track. Use `snake_case` or separate words with '`.`'.

```{r}
#| label: rename
#| eval: true

## Tidyverse method
# The syntax is rename(new_name = old_name)
df <- df |>
  rename(
    murder = `Homicide..Murder.`,
    UEWI_dwell = `Unlawful.Entry.Without.Violence...Dwelling`,
    UEWI_dwell_vio = `Unlawful.Entry.With.Violence...Dwelling`
  )

# Display the first few row with the new names
head(df, n=1)


## Base R Method
# Display all of the current colnames in your console
colnames(df)

# Explicitly rename the 5th column
colnames(df)[5]
colnames(df)[5] <- "oth_homic"
colnames(df)[5]

# Explicitly rename columns 1 through to three
colnames(df)[1:3]
colnames(df)[1:3] <- c("id", "lga", "my")
colnames(df)[1:3]
```

### Formatting Dates

This is the stage of the process that has caused me the most grief in the past and is usually where I come unstuck today. Covering all of the different date formats is beyond the scope of this page. However, there are an abundance of guides. If you use the `lubridate` package, you can get the list of arguments and formats by typing `??lubridate` or `help(package = "lubridate")` into your console.

```{r}
#| label: formatting dates
#| eval: true

# First, ensure the date column is in the correct format using lubridate
df <- df |>
  mutate(
    # mdy() parses dates in Month-Day-Year format. Use other functions 
    # like dmy() or ymd() depending on your data.
    # for this data set we will use my because our unformated data is like JAN01
    offence_date = lubridate::my(my)  # 
  )
```

**Note**: We created a new date variable offence_date.

### Create Conditional Columns

You might need to create new variables based on existing data. A common task is creating an "experimental" vs. "control" group based on a date, or classifying time periods. In this example, we use `mutate()` combined with the `if_else` function. I also demonstrate `ifelse()` function to highlight an alternative method. It is worth noting that `if_else()` only handles two conditions. For the sake of brevity, we won't cover all of the different options for multiple conditions here. However, if you would like to know how to handle the creation of multiple conditions (e.g 'pre', 'during', 'post'), look to combine the `mutate()` and `case_when` functions.

Here, we'll create a `period` column to distinguish between data before and after an intervention date (e.g., July 1st, 2020).

```{r}
#| label: mutate
#| eval: true

# Now, create the conditional column
df <- df |>
  dplyr::mutate(
    intervention_period = if_else(
      offence_date >= ymd("2020-07-01"),
      "Post-Intervention",
      "Pre-Intervention"
    )
  )

# An alternative method
df$intervention_period <- ifelse(df$offence_date >= "2020-07-01", 
                        "Post", "Pre")

# Check the result
unique(df$intervention_period) # Idnetifies the unique values for a given column
```

**Note**: As I have loaded additional packages to complete various analytical processes, I have found that some functions need to be called explicitly from their owning package to work effectively. I have found that I commonly need to do this for functions from the `dplyr` package. For example, in the code chunk above, I have explicitly called the mutate functions by typing `dplyr::mutate(...)` as opposed to `mutate(...)`.

------------------------------------------------------------------------

### Select Columns and Filter Rows

You rarely need all the columns or rows from the original dataset, or you may want to focus on a subset of your data.

```{r}
#| label: select-filter
#| eval: true

# Let's keep only the columns we need and filter for a specific offence type
ipswich_lga <- df |>
  select(lga, offence_date, UEWI_dwell_vio, intervention_period) |> # select the specific column you want to keep
  filter(lga == "Ipswich City Council") # filters the rows based on the variables within a column

# Check results
head(ipswich_lga, n = 10)
```

## Basic Visualisation with `ggplot2`

A picture is worth a thousand words and is a fundamental tool for communicating the results of any analytical process. **ggplot2** is definitely my preferred tool for data visualisation in R.

### A Basic Bar Chart

In our initial example, let's explore how frequently burglary with violence occurs in Queensland Local Government Areas (LGAs) by visualising the number of recorded offences per month.

```{r}
#| label: basic-plot
#| eval: false

ggplot(data = df, aes(x = UEWI_dwell_vio)) +
  geom_bar() +
  labs(
    title = "Frequency of Monthly Violent Burglaries Across Queensland LGAs",
    x = "Number of Offences Committed within LGAs",
    y = "Number of Months"
  ) +
  theme_minimal()
```

### A Bar Chart with X and Y Variables

This type of chart is perfect for showing summarised data. You typically create a summary table first, then plot that. This is best done using `geom_col()`.

```{r}
#| label: bar chcart with x and y

ggplot(data = ipswich_lga, aes(x = offence_date, y = UEWI_dwell_vio)) +
  geom_col() +
  labs(
    title = "Frequency of Violent Burglaries per Month",
    x = "Month Year",
    y = "Number of Violent Burglaries"
  ) +
  theme_minimal()
```

### Piped ggplot

The code format displayed for ggplot is precisely how I first learnt R. However, as time has progressed, I have preferred to pipe my data into ggplot, allowing me to include additional pipes to visualise my data without creating another distinct data frame.

```{r}
#| label: piped ggplot
#| eval: true

df |> 
  filter(lga == "Brisbane City Council") |> # Added filter to summarise the data for Brisbane, rather than create a new df
  ggplot(aes(offence_date, UEWI_dwell_vio)) + 
  geom_col() +
  labs(
    title = "Frequency of Violent Burglaries per Month",
    x = "Month Year",
    y = "Number of Violent Burglaries"
  ) +
  theme_minimal()
```

------------------------------------------------------------------------

### A Piped Chart with a Legend

To add more detail, you can map a variable to an aesthetic like `fill` for bar charts or `color` for line charts. This will automatically generate a legend. Let's create a new variable for the day of the week and see how offences are distributed across periods.

```{r}
#| label: legend-plot
#| eval: true


df |> 
  filter(lga == "Brisbane City Council" | lga == "Ipswich City Council") |> # Filter is now looking for Brisbane OR Ipswich City Council data
  ggplot(aes(offence_date, UEWI_dwell_vio, fill = lga)) + 
  geom_col(position = "dodge") + # Seperate the bars, as opposed to stacked
  labs(
    title = "Frequency of Violent Burglaries per Month",
    x = "Month Year",
    y = "Number of Violent Burglaries",
    fill = "LGA" # Renames the legend from "lga"
  ) +
  theme_minimal()
```

### Aggregating and Plotting Data with Pipes

At the start of this section, we created a basic bar chart exploring the frequency of violent burglary counts in LGAs across Queensland. This time, we are going to use pipes `|>` to aggregate violent burglary counts for all of Queensland to visualise the distribution of offence counts across Queensland.

```{r}
#| label: chart-with-multiple-pipes

df |>
  group_by(offence_date) |> # group the data by month year, irrespective of the LGA
  summarise(count = sum(UEWI_dwell_vio)) |> # aggregate the violent burglary counts, createing a new variable name 'count'
  ggplot(aes(count)) + # plot the new 'count' variable.
  geom_bar() +
  labs(
    title = "Frequency of Monthly Violent Burglary Counts in Queensland",
    x = "Count of Monthly Offences",
    y = "Number of Months"
  )
  

```

## Dynamic In-text Reporting

Quarto allows you to embed R code directly into your text. This is perfect for reporting summary statistics that automatically update if your data changes.

To do this, you enclose R code in back ticks like this: \`r `format(Sys.Date(), "%Y")`\` displays `` `r format(Sys.Date(), "%Y")` ``.

### Example

The dataset contains a total of `r nrow(df)` rows of data. The date range for this data is from \`r `min(df$offence_date, na.rm = TRUE`\` **`r min(df$offence_date, na.rm = TRUE)`** to \`r `max(df$offence_date, na.rm = TRUE`\` **`r max(df$offence_date, na.rm = TRUE)`**.

------------------------------------------------------------------------

## Useful Analytical Packages

Providing a detailed review of the different analytical packages I have used or experimented with is well beyond the scope of this guide. However, I can provide you with a list of packages I have used regularly, in an effort to get you started.

-   The `MASS` package has been useful for deploying non-parametric regression models.

-   The `fpp3` package, and accompanying ebook, has helped me with time series analysis

-   The `sf` and `tmap` packages are great for mapping spatial data

-   The `pglm` package has been useful for running panel generalised linear models.

## References
